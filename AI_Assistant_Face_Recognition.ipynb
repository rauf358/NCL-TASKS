{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd72fab9",
   "metadata": {},
   "source": [
    "# AI Assistant with Face Recognition\n",
    "\n",
    "This Jupyter Notebook contains an AI assistant that uses OpenCV, MediaPipe, and DeepFace to detect faces, recognize users, and assist them via voice commands.\n",
    "\n",
    "Each function is placed in a separate cell for clarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82dacfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "\n",
    "# Initialize voice engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "def speak(text):\n",
    "    \"\"\"Converts text to speech.\"\"\"\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12a5678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "def get_voice_command():\n",
    "    \"\"\"Captures and recognizes a voice command from the user.\"\"\"\n",
    "    with sr.Microphone() as source:\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        speak(\"Listening for your command...\")\n",
    "        try:\n",
    "            audio = recognizer.listen(source, timeout=5)\n",
    "            command = recognizer.recognize_google(audio).lower()\n",
    "            return command\n",
    "        except sr.UnknownValueError:\n",
    "            return \"\"\n",
    "        except sr.RequestError:\n",
    "            return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ef40160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assist_user():    \n",
    "    # Ask the user how they need help\n",
    "    prompt = \"How can I help you? Please choose an option: \\n1. Meet Dr. Hashim \\n2. Meet Mr. Misbah \\n3. Meet Mr. Taha\"\n",
    "    print(prompt)\n",
    "    engine.say(\"How can I help you? Please choose an option.\")\n",
    "    engine.runAndWait()\n",
    "    engine.say(\"Press 1 to meet Doctor Hashim, 2 for Mr. Misbah, or 3 for Mr. Taha.\")\n",
    "    engine.runAndWait()\n",
    "    \n",
    "    # Get user input\n",
    "    choice = input(\"Enter your choice (1/2/3): \").strip()\n",
    "    \n",
    "    # Process the choice\n",
    "    if choice == \"1\":\n",
    "        response = \"You have chosen to meet Dr. Hashim.\"\n",
    "    elif choice == \"2\":\n",
    "        response = \"You have chosen to meet Mr. Misbah.\"\n",
    "    elif choice == \"3\":\n",
    "        response = \"You have chosen to meet Mr. Taha.\"\n",
    "    else:\n",
    "        response = \"Invalid choice. Please try again.\"\n",
    "    \n",
    "    # Speak the response\n",
    "  \n",
    "    engine.say(response)\n",
    "    engine.runAndWait()\n",
    "    print(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6072d378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import json\n",
    "\n",
    "USER_DATA_FILE = \"user_data.json\"\n",
    "\n",
    "if os.path.exists(USER_DATA_FILE):\n",
    "    with open(USER_DATA_FILE, \"r\") as f:\n",
    "        user_data = json.load(f)\n",
    "else:\n",
    "    user_data = {}\n",
    "\n",
    "def capture_new_face(face_image):\n",
    "    \"\"\"Captures and stores a new user's face with their name.\"\"\"\n",
    "    speak(\"Who are you? Please enter your name.\")\n",
    "    name = input(\"Please enter your name: \")\n",
    "    save_dir = r\"F:\\Buffer\\Applied data science and AI specialization\\NCL open cv tasks\\image database\"\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Ensure directory exists\n",
    "    save_path = os.path.join(save_dir, f\"{name}.jpg\")\n",
    "\n",
    "    cv2.imwrite(save_path, face_image)\n",
    "    user_data[name] = {\"greet_count\": 0}\n",
    "    with open(USER_DATA_FILE, \"w\") as f:\n",
    "        json.dump(user_data, f)\n",
    "    speak(f\"Image saved. Welcome, {name}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96aefa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\anaconda\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from deepface import DeepFace\n",
    "\n",
    "def detect_emotion(face_image):\n",
    "    \"\"\"Analyzes the detected face for emotion.\"\"\"\n",
    "    try:\n",
    "        result = DeepFace.analyze(face_image, actions=[\"emotion\"], enforce_detection=False)\n",
    "        if result:\n",
    "            return result[0]['dominant_emotion']\n",
    "    except:\n",
    "        return \"neutral\"\n",
    "    return \"neutral\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab506164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects(frame):\n",
    "    \"\"\"Detects objects in the given frame using YOLO.\"\"\"\n",
    "    net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    \n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "    \n",
    "    objects_detected = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = scores.argmax()\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                objects_detected.append(class_id)\n",
    "    \n",
    "    return objects_detected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41eb85d5-3e51-41bc-a0d1-c625d80f66f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import threading\n",
    "from deepface import DeepFace\n",
    "\n",
    "def find_face():\n",
    "    try:\n",
    "        results = DeepFace.find(\n",
    "            img_path='temp_face.jpg',\n",
    "            db_path=r\"F:\\Buffer\\Applied data science and AI specialization\\NCL open cv tasks\\image database\",\n",
    "            model_name=\"Facenet\",\n",
    "            enforce_detection=False\n",
    "        )\n",
    "\n",
    "        if results and len(results[0]) > 0:\n",
    "            name = os.path.basename(results[0]['identity'][0]).split('.')[0]\n",
    "\n",
    "            if name in user_data:\n",
    "                user_data[name]['greet_count'] += 1\n",
    "            else:\n",
    "                user_data[name] = {\"greet_count\": 1}\n",
    "\n",
    "            with open(USER_DATA_FILE, \"w\") as f:\n",
    "                json.dump(user_data, f)\n",
    "\n",
    "            speak(f\"Welcome back, {name}!\")\n",
    "            emotion = detect_emotion(face_image)\n",
    "\n",
    "            if emotion == \"sad\":\n",
    "                speak(\"You seem a bit down today. Can I do something for you?\")\n",
    "            elif emotion == \"happy\":\n",
    "                speak(\"You look happy! Glad to see you smiling!\")\n",
    "\n",
    "            threading.Thread(target=assist_user).start()\n",
    "        else:\n",
    "            speak(\"Face not recognized. Who are you?\")\n",
    "            threading.Thread(target=capture_new_face, args=(face_image,)).start()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        speak(\"An error occurred while recognizing your face.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b35b842b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25-03-21 11:43:34 - Searching temp_face.jpg in 4 length datastore\n",
      "25-03-21 11:43:59 - find function duration 24.906829595565796 seconds\n",
      "How can I help you? Please choose an option: \n",
      "1. Meet Dr. Hashim \n",
      "2. Meet Mr. Misbah \n",
      "3. Meet Mr. Taha\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (1/2/3):  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have chosen to meet Mr. Taha.\n",
      "25-03-21 11:44:34 - Searching temp_face.jpg in 4 length datastore\n",
      "25-03-21 11:44:34 - find function duration 0.7455530166625977 seconds\n",
      "How can I help you? Please choose an option: \n",
      "1. Meet Dr. Hashim \n",
      "2. Meet Mr. Misbah \n",
      "3. Meet Mr. Taha\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import time\n",
    "import threading\n",
    "\n",
    "# Initialize Face Detection\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.5)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "start_time = None\n",
    "face_detected = False\n",
    "greeted = False\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    small_frame = cv2.resize(frame, (640, 480))\n",
    "    rgb_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(rgb_frame)\n",
    "    \n",
    "    if results.detections:\n",
    "        for detection in results.detections:\n",
    "            bboxC = detection.location_data.relative_bounding_box\n",
    "            h, w, _ = small_frame.shape\n",
    "            x, y, w, h = int(bboxC.xmin * w), int(bboxC.ymin * h), int(bboxC.width * w), int(bboxC.height * h)\n",
    "            cv2.rectangle(small_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        \n",
    "        if not face_detected:\n",
    "            start_time = time.time()\n",
    "            face_detected = True\n",
    "        \n",
    "        time_passed = time.time() - start_time\n",
    "        if time_passed > 3.0 and not greeted:\n",
    "            greeted = True\n",
    "            face_image = small_frame[y:y+h, x:x+w]\n",
    "            cv2.imwrite(\"temp_face.jpg\",frame)  # Always overwrite this file\n",
    "            threading.Thread(target=find_face).start()\n",
    "    else:\n",
    "        start_time = None\n",
    "        face_detected = False\n",
    "        greeted = False\n",
    "    \n",
    "    cv2.imshow(\"AI Assistant\", small_frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e11a230-a9d9-487b-8057-5d3dfe1cc79f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cb9831-a90f-4c66-a586-0952473f51c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
